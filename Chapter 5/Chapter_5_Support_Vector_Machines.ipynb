{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04ede64",
   "metadata": {},
   "source": [
    "# Chapter 5 - Support Vector Machines\n",
    "\n",
    "This notebook covers Support Vector Machines (SVMs), including:\n",
    "- Linear SVM Classification\n",
    "- Nonlinear SVM Classification\n",
    "- SVM Regression\n",
    "- The math behind SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a634eea",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b70799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"svm\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146afbfc",
   "metadata": {},
   "source": [
    "## Linear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e755b",
   "metadata": {},
   "source": [
    "### Large Margin Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916bcb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n",
    "    ])\n",
    "\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003102aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svm_clf.predict([[5.5, 1.7]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e2b58",
   "metadata": {},
   "source": [
    "Unlike Logistic Regression classifiers, SVM classifiers do not output probabilities for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652d914",
   "metadata": {},
   "source": [
    "Alternatively, you could use the `SVC` class, using `SVC(kernel=\"linear\", C=1)`, but it is much slower, especially with large training sets, so it is not recommended. Another option is to use the `SGDClassifier` class, with `SGDClassifier(loss=\"hinge\", alpha=1/(m*C))`. This applies regular Stochastic Gradient Descent to train a linear SVM classifier. It does not converge as fast as the `LinearSVC` class, but it can be useful to handle online classification tasks or huge datasets that do not fit in memory (out-of-core training)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c14c34",
   "metadata": {},
   "source": [
    "### Soft Margin Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad example: this is a noisy dataset\n",
    "X_outliers = np.random.rand(50, 2)\n",
    "y_outliers = (X_outliers.sum(axis=1) > 1).astype(np.float64)\n",
    "X_outliers = X_outliers * 4 - 2\n",
    "\n",
    "plt.figure(figsize=(5.5, 3.2))\n",
    "plt.plot(X_outliers[:, 0][y_outliers==1], X_outliers[:, 1][y_outliers==1], \"bs\")\n",
    "plt.plot(X_outliers[:, 0][y_outliers==0], X_outliers[:, 1][y_outliers==0], \"yo\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "plt.title(\"Impossible to separate linearly\", fontsize=14)\n",
    "plt.axis([-2.5, 2.5, -2.5, 2.5])\n",
    "save_fig(\"impossible_to_separate_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f7d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the iris dataset\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0077985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "scaler = StandardScaler()\n",
    "svm_clf1 = LinearSVC(C=1, loss=\"hinge\", random_state=42)\n",
    "svm_clf2 = LinearSVC(C=100, loss=\"hinge\", random_state=42)\n",
    "\n",
    "scaled_svm_clf1 = Pipeline([\n",
    "        (\"scaler\", scaler),\n",
    "        (\"linear_svc\", svm_clf1),\n",
    "    ])\n",
    "scaled_svm_clf2 = Pipeline([\n",
    "        (\"scaler\", scaler),\n",
    "        (\"linear_svc\", svm_clf2),\n",
    "    ])\n",
    "\n",
    "scaled_svm_clf1.fit(X, y)\n",
    "scaled_svm_clf2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to unscaled parameters\n",
    "b1 = svm_clf1.decision_function([-scaler.mean_ / scaler.scale_])\n",
    "b2 = svm_clf2.decision_function([-scaler.mean_ / scaler.scale_])\n",
    "w1 = svm_clf1.coef_[0] / scaler.scale_\n",
    "w2 = svm_clf2.coef_[0] / scaler.scale_\n",
    "svm_clf1.intercept_ = np.array([b1])\n",
    "svm_clf2.intercept_ = np.array([b2])\n",
    "svm_clf1.coef_ = np.array([w1])\n",
    "svm_clf2.coef_ = np.array([w2])\n",
    "\n",
    "# Find support vectors (LinearSVC does not do this automatically)\n",
    "t = y * 2 - 1\n",
    "support_vectors_idx1 = (t * (X.dot(w1) + b1) < 1).ravel()\n",
    "support_vectors_idx2 = (t * (X.dot(w2) + b2) < 1).ravel()\n",
    "svm_clf1.support_vectors_ = X[support_vectors_idx1]\n",
    "svm_clf2.support_vectors_ = X[support_vectors_idx2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e0626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_boundary(svm_clf, xmin, xmax):\n",
    "    w = svm_clf.coef_[0]\n",
    "    b = svm_clf.intercept_[0]\n",
    "\n",
    "    # At the decision boundary, w0*x0 + w1*x1 + b = 0\n",
    "    # => x1 = -w0/w1 * x0 - b/w1\n",
    "    x0 = np.linspace(xmin, xmax, 200)\n",
    "    decision_boundary = -w[0]/w[1] * x0 - b/w[1]\n",
    "\n",
    "    margin = 1/w[1]\n",
    "    gutter_up = decision_boundary + margin\n",
    "    gutter_down = decision_boundary - margin\n",
    "\n",
    "    svs = svm_clf.support_vectors_\n",
    "    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')\n",
    "    plt.plot(x0, decision_boundary, \"k-\", linewidth=2)\n",
    "    plt.plot(x0, gutter_up, \"k--\", linewidth=2)\n",
    "    plt.plot(x0, gutter_down, \"k--\", linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7303081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\", label=\"Iris virginica\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Iris versicolor\")\n",
    "plot_svc_decision_boundary(svm_clf1, 4, 5.9)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.title(\"$C = {}$\".format(svm_clf1.C), fontsize=16)\n",
    "plt.axis([4, 5.9, 0.8, 2.8])\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "plot_svc_decision_boundary(svm_clf2, 4, 5.99)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.title(\"$C = {}$\".format(svm_clf2.C), fontsize=16)\n",
    "plt.axis([4, 5.9, 0.8, 2.8])\n",
    "\n",
    "save_fig(\"regularization_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21882119",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a9b53",
   "metadata": {},
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75131b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
    "\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "save_fig(\"moons_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f3f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_svm_clf = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n",
    "    ])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "\n",
    "def plot_predictions(clf, axes):\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X = np.c_[x0.ravel(), x1.ravel()]\n",
    "    y_pred = clf.predict(X).reshape(x0.shape)\n",
    "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
    "\n",
    "plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "save_fig(\"moons_polynomial_svc_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c451c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "    ])\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b594d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(\"Polynomial Features\", fontsize=14)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_predictions(poly_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(\"Polynomial Kernel\", fontsize=14)\n",
    "\n",
    "save_fig(\"moons_kernelized_polynomial_svc_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad4e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma1, gamma2 = 0.1, 5\n",
    "C1, C2 = 0.001, 1000\n",
    "degree1, degree2, degree3 = 1, 2, 3\n",
    "coef01, coef02 = 0, 10\n",
    "\n",
    "poly_kernel_svm_clf1 = SVC(kernel=\"poly\", degree=degree1, coef0=coef01, C=C1)\n",
    "poly_kernel_svm_clf2 = SVC(kernel=\"poly\", degree=degree2, coef0=coef02, C=C2)\n",
    "\n",
    "poly_kernel_svm_clf1.fit(X, y)\n",
    "poly_kernel_svm_clf2.fit(X, y)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plot_predictions(poly_kernel_svm_clf1, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(r\"$d={}, r={}, C={}$\".format(degree1, coef01, C1), fontsize=18)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_predictions(poly_kernel_svm_clf2, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(r\"$d={}, r={}, C={}$\".format(degree2, coef02, C2), fontsize=18)\n",
    "\n",
    "save_fig(\"moons_kernelized_polynomial_svc_plot_with_different_hyperparams\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07504671",
   "metadata": {},
   "source": [
    "### Adding Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cea6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_rbf(x, landmark, gamma):\n",
    "    return np.exp(-gamma * np.linalg.norm(x - landmark, axis=1)**2)\n",
    "\n",
    "gamma = 0.3\n",
    "\n",
    "x1s = np.linspace(-4.5, 4.5, 200).reshape(-1, 1)\n",
    "x2s = gaussian_rbf(x1s, -2, gamma)\n",
    "x3s = gaussian_rbf(x1s, 1, gamma)\n",
    "\n",
    "XK = np.c_[gaussian_rbf(X1D, -2, gamma), gaussian_rbf(X1D, 1, gamma)]\n",
    "yk = np.array([0, 0, 1, 1, 1, 1, 0, 0])\n",
    "\n",
    "plt.figure(figsize=(10.5, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.scatter(x1s[yk==0], [0]*np.sum(yk==0), c=\"bs\")\n",
    "plt.scatter(x1s[yk==1], [0]*np.sum(yk==1), c=\"g^\")\n",
    "plt.plot([-4.5, 4.5], [0, 0], \"k-\")\n",
    "plt.ylabel(r\"Similarity\", fontsize=14)\n",
    "plt.xlabel(r\"$x_1$\", fontsize=14)\n",
    "plt.title(r\"1D dataset\", fontsize=14)\n",
    "plt.axis([-4.5, 4.5, -0.1, 1.1])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.grid(True, which='both')\n",
    "plt.plot(x2s, x3s, \"r-\", linewidth=3)\n",
    "plt.plot(XK[yk==0, 0], XK[yk==0, 1], \"bs\")\n",
    "plt.plot(XK[yk==1, 0], XK[yk==1, 1], \"g^\")\n",
    "plt.xlabel(r\"$x_2$ (similarity to $l_1 = -2$)\", fontsize=14)\n",
    "plt.ylabel(r\"$x_3$ (similarity to $l_2 = 1$)\", fontsize=14, rotation=90)\n",
    "plt.title(r\"Transformed dataset (2D)\", fontsize=14)\n",
    "plt.axis([0, 1.1, 0, 1.1])\n",
    "\n",
    "plt.subplots_adjust(right=1)\n",
    "\n",
    "save_fig(\"kernel_method_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d396c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1D = np.linspace(-4, 4, 9).reshape(-1, 1)\n",
    "X2D = np.c_[X1D, X1D**2]\n",
    "y = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n",
    "\n",
    "plt.figure(figsize=(10.5, 3.2))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.plot(X1D[:, 0][y==0], np.zeros(4), \"bs\")\n",
    "plt.plot(X1D[:, 0][y==1], np.zeros(5), \"g^\")\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.xlabel(r\"$x_1$\", fontsize=14)\n",
    "plt.axis([-4.5, 4.5, -0.2, 0.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.grid(True, which='both')\n",
    "plt.plot(X2D[:, 0][y==0], X2D[:, 1][y==0], \"bs\")\n",
    "plt.plot(X2D[:, 0][y==1], X2D[:, 1][y==1], \"g^\")\n",
    "plt.xlabel(r\"$x_1$\", fontsize=14)\n",
    "plt.ylabel(r\"$x_2$\", fontsize=14, rotation=0)\n",
    "plt.gca().get_yaxis().set_ticks([0, 4, 8, 12, 16])\n",
    "plt.plot([-4.5, 4.5], [6.5, 6.5], \"r--\", linewidth=3)\n",
    "plt.axis([-4.5, 4.5, -1, 17])\n",
    "\n",
    "plt.subplots_adjust(right=1)\n",
    "\n",
    "save_fig(\"higher_dimensions_plot\", tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ac167",
   "metadata": {},
   "source": [
    "### Gaussian RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd26df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "    ])\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef0325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "gamma1, gamma2 = 0.1, 5\n",
    "C1, C2 = 0.001, 1000\n",
    "\n",
    "hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n",
    "\n",
    "svm_clfs = []\n",
    "for gamma, C in hyperparams:\n",
    "    rbf_kernel_svm_clf = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n",
    "        ])\n",
    "    rbf_kernel_svm_clf.fit(X, y)\n",
    "    svm_clfs.append(rbf_kernel_svm_clf)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)\n",
    "\n",
    "for i, (gamma, C) in enumerate(hyperparams):\n",
    "    plt.sca(axes[i // 2, i % 2])\n",
    "    plot_predictions(svm_clfs[i], [-1.5, 2.4, -1, 1.5])\n",
    "    plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\n",
    "    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=16)\n",
    "    if i in (0, 1):\n",
    "        plt.xlabel(\"\")\n",
    "    if i in (1, 3):\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "save_fig(\"moons_rbf_svc_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd2356",
   "metadata": {},
   "source": [
    "## SVM Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f75f3",
   "metadata": {},
   "source": [
    "### Linear SVM Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 50\n",
    "X = 2 * np.random.rand(m, 1)\n",
    "y = (4 + 3 * X + np.random.randn(m, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45077b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svm_reg = LinearSVR(epsilon=1.5, random_state=42)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17f373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_reg1 = LinearSVR(epsilon=1.5, random_state=42)\n",
    "svm_reg2 = LinearSVR(epsilon=0.5, random_state=42)\n",
    "svm_reg1.fit(X, y)\n",
    "svm_reg2.fit(X, y)\n",
    "\n",
    "def find_support_vectors(svm_reg, X, y):\n",
    "    y_pred = svm_reg.predict(X)\n",
    "    off_margin = (np.abs(y - y_pred) >= svm_reg.epsilon)\n",
    "    return np.argwhere(off_margin)\n",
    "\n",
    "svm_reg1.support_ = find_support_vectors(svm_reg1, X, y)\n",
    "svm_reg2.support_ = find_support_vectors(svm_reg2, X, y)\n",
    "\n",
    "eps_x1 = 1\n",
    "eps_y_pred = svm_reg1.predict([[eps_x1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c48adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_regression(svm_reg, X, y, axes):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)\n",
    "    y_pred = svm_reg.predict(x1s)\n",
    "    plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "    plt.plot(x1s, y_pred + svm_reg.epsilon, \"k--\", linewidth=2)\n",
    "    plt.plot(x1s, y_pred - svm_reg.epsilon, \"k--\", linewidth=2)\n",
    "    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors='#FFAAAA')\n",
    "    plt.plot(X, y, \"bo\")\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=14)\n",
    "    plt.legend(loc=\"upper left\", fontsize=14)\n",
    "    plt.axis(axes)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_svm_regression(svm_reg1, X, y, [0, 2, 3, 11])\n",
    "plt.title(r\"$\\epsilon = {}$\".format(svm_reg1.epsilon), fontsize=18)\n",
    "plt.ylabel(r\"$y$\", fontsize=14, rotation=0)\n",
    "#plt.plot([eps_x1, eps_x1], [eps_y_pred, eps_y_pred - svm_reg1.epsilon], \"k-\", linewidth=2)\n",
    "plt.annotate('', xy=(eps_x1, eps_y_pred), xytext=(eps_x1, eps_y_pred - svm_reg1.epsilon),\n",
    "            arrowprops=dict(arrowstyle='<->', connectionstyle='arc3', color='k', lw=1.5))\n",
    "plt.text(0.91, 5.6, r\"$\\epsilon$\", fontsize=20)\n",
    "plt.sca(axes[1])\n",
    "plot_svm_regression(svm_reg2, X, y, [0, 2, 3, 11])\n",
    "plt.title(r\"$\\epsilon = {}$\".format(svm_reg2.epsilon), fontsize=18)\n",
    "save_fig(\"svm_regression_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47128bd4",
   "metadata": {},
   "source": [
    "### Nonlinear SVM Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4466771",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 2 * np.random.rand(m, 1) - 1\n",
    "y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a0727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly_reg1 = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\n",
    "svm_poly_reg2 = SVR(kernel=\"poly\", degree=2, C=0.01, epsilon=0.1, gamma=\"scale\")\n",
    "svm_poly_reg1.fit(X, y)\n",
    "svm_poly_reg2.fit(X, y)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_svm_regression(svm_poly_reg1, X, y, [-1, 1, 0, 1])\n",
    "plt.title(r\"$degree={}, C={}, \\epsilon = {}$\".format(svm_poly_reg1.degree, svm_poly_reg1.C, svm_poly_reg1.epsilon), fontsize=18)\n",
    "plt.ylabel(r\"$y$\", fontsize=14, rotation=0)\n",
    "plt.sca(axes[1])\n",
    "plot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])\n",
    "plt.title(r\"$degree={}, C={}, \\epsilon = {}$\".format(svm_poly_reg2.degree, svm_poly_reg2.C, svm_poly_reg2.epsilon), fontsize=18)\n",
    "save_fig(\"svm_with_polynomial_kernel_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3a62d",
   "metadata": {},
   "source": [
    "## Under the Hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9facbe5",
   "metadata": {},
   "source": [
    "This section explains the mathematical details of how SVMs work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad8732a",
   "metadata": {},
   "source": [
    "### Decision Function and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75848af",
   "metadata": {},
   "source": [
    "The linear SVM classifier model predicts the class of a new instance **x** by simply computing the decision function $\\mathbf{w}^T \\mathbf{x} + b = w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n + b$: if the result is positive, the predicted class $\\hat{y}$ is the positive class (1), or else it is the negative class (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce9fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n",
    "\n",
    "svm_clf = SVC(kernel=\"linear\", C=float(\"inf\"))\n",
    "svm_clf.fit(X, y)\n",
    "\n",
    "# Bad example: this is a noisy dataset\n",
    "X_outliers = np.array([[3.4, 1.3], [3.2, 0.8]])\n",
    "y_outliers = np.array([0, 0])\n",
    "X_outliers_plot = np.array([[3.4, 1.3], [3.2, 0.8]])\n",
    "y_outliers_plot = np.array([0, 0])\n",
    "\n",
    "svm_clf_without_outliers = SVC(kernel=\"linear\", C=float(\"inf\"))\n",
    "svm_clf_without_outliers.fit(X, y)\n",
    "\n",
    "svm_clf_with_outliers = SVC(kernel=\"linear\", C=float(\"inf\"))\n",
    "svm_clf_with_outliers.fit(np.vstack([X, X_outliers]), np.hstack([y, y_outliers]))\n",
    "\n",
    "def plot_svc_decision_boundary(svm_clf, xmin, xmax):\n",
    "    w = svm_clf.coef_[0]\n",
    "    b = svm_clf.intercept_[0]\n",
    "\n",
    "    # At the decision boundary, w0*x0 + w1*x1 + b = 0\n",
    "    # => x1 = -w0/w1 * x0 - b/w1\n",
    "    x0 = np.linspace(xmin, xmax, 200)\n",
    "    decision_boundary = -w[0]/w[1] * x0 - b/w[1]\n",
    "\n",
    "    margin = 1/w[1]\n",
    "    gutter_up = decision_boundary + margin\n",
    "    gutter_down = decision_boundary - margin\n",
    "\n",
    "    svs = svm_clf.support_vectors_\n",
    "    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')\n",
    "    plt.plot(x0, decision_boundary, \"k-\", linewidth=2)\n",
    "    plt.plot(x0, gutter_up, \"k--\", linewidth=2)\n",
    "    plt.plot(x0, gutter_down, \"k--\", linewidth=2)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\", label=\"Iris virginica\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Iris versicolor\")\n",
    "plot_svc_decision_boundary(svm_clf_without_outliers, 4, 5.9)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.title(\"Unscaled\", fontsize=16)\n",
    "plt.axis([4, 5.9, 0.8, 2.8])\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "plot_svc_decision_boundary(svm_clf_with_outliers, 4, 5.9)\n",
    "plt.plot(X_outliers_plot[:, 0], X_outliers_plot[:, 1], \"yo\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.title(\"Impossible to separate\", fontsize=16)\n",
    "plt.axis([4, 5.9, 0.8, 2.8])\n",
    "\n",
    "save_fig(\"sensitivity_to_outliers_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081efabb",
   "metadata": {},
   "source": [
    "### Training Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa99bad",
   "metadata": {},
   "source": [
    "The objective of the SVM algorithm is to find the values of $\\mathbf{w}$ and $b$ that make this margin as wide as possible while limiting the margin violations (i.e., instances that end up in the middle of the margin or even on the wrong side)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f063b",
   "metadata": {},
   "source": [
    "### Quadratic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfeb7a6",
   "metadata": {},
   "source": [
    "The hard margin and soft margin problems are both convex quadratic optimization problems with linear constraints. Such problems are known as Quadratic Programming (QP) problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde4ae79",
   "metadata": {},
   "source": [
    "### The Dual Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53571baf",
   "metadata": {},
   "source": [
    "Given a constrained optimization problem, known as the primal problem, it is possible to express a different but closely related problem, called its dual problem. The solution to the dual problem typically gives a lower bound to the solution of the primal problem, but under some conditions it can even have the same solution as the primal problem. Luckily, the SVM problem happens to meet these conditions, so you can choose to solve the primal problem or the dual problem; both will have the same solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833f036d",
   "metadata": {},
   "source": [
    "### Kernelized SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d3780",
   "metadata": {},
   "source": [
    "Suppose you want to apply a 2nd-degree polynomial transformation to a 2D training set (such as the moons dataset), then train a linear SVM classifier on the transformed training set. The equation looks like this:\n",
    "\n",
    "$\\phi\\left(\\mathbf{x}\\right) = \\phi\\left(\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{pmatrix}\\right) = \\begin{pmatrix}\n",
    "{x_1}^2 \\\\\n",
    "\\sqrt{2} \\, x_1 x_2 \\\\\n",
    "{x_2}^2\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32142249",
   "metadata": {},
   "source": [
    "### Online SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ac638",
   "metadata": {},
   "source": [
    "Before concluding this chapter, let's take a quick look at online SVM classifiers (recall that online learning means learning incrementally, typically as new instances arrive).\n",
    "\n",
    "For linear SVM classifiers, one method is to use Gradient Descent (e.g., using `SGDClassifier`) to minimize the cost function, which is derived from the primal problem. Unfortunately, Gradient Descent converges much more slowly than the methods based on QP.\n",
    "\n",
    "For large-scale nonlinear problems, you may want to consider using neural networks instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2695b662",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter, we covered:\n",
    "\n",
    "1. **Linear SVM Classification**: Understanding margin maximization and support vectors\n",
    "2. **Soft Margin Classification**: Handling non-linearly separable data with the C parameter\n",
    "3. **Nonlinear Classification**: Using polynomial and RBF kernels\n",
    "4. **SVM Regression**: Applying SVMs to regression problems\n",
    "5. **Mathematical Foundation**: Understanding the dual problem and kernelization\n",
    "\n",
    "**Key takeaways:**\n",
    "- SVMs are powerful for both classification and regression\n",
    "- They work well with high-dimensional data\n",
    "- Kernel trick allows handling nonlinear problems efficiently\n",
    "- Feature scaling is crucial for SVM performance\n",
    "- SVMs are memory efficient (only store support vectors)\n",
    "\n",
    "**When to use SVMs:**\n",
    "- Small to medium-sized datasets\n",
    "- High-dimensional data\n",
    "- Clear margin of separation exists\n",
    "- Memory efficiency is important\n",
    "\n",
    "**Limitations:**\n",
    "- Poor performance on large datasets\n",
    "- Sensitive to feature scaling\n",
    "- No probabilistic output (by default)\n",
    "- Sensitive to outliers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
