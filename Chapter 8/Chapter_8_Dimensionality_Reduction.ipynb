{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5bcbe75",
   "metadata": {},
   "source": [
    "# Chapter 8 - Dimensionality Reduction\n",
    "\n",
    "This notebook covers dimensionality reduction techniques, including:\n",
    "- The Curse of Dimensionality\n",
    "- Main Approaches for Dimensionality Reduction\n",
    "- Principal Component Analysis (PCA)\n",
    "- Kernel PCA\n",
    "- Locally Linear Embedding (LLE)\n",
    "- Other Dimensionality Reduction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46351e0c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97576f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"dim_reduction\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc7e4f",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7dc2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the curse of dimensionality\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate random points in different dimensions\n",
    "np.random.seed(42)\n",
    "m = 1000\n",
    "dimensions = [1, 2, 3, 5, 10, 20, 50, 100]\n",
    "distances = []\n",
    "\n",
    "for d in dimensions:\n",
    "    # Generate random points in d-dimensional space\n",
    "    X = np.random.rand(m, d)\n",
    "    \n",
    "    # Calculate distances from first point to all others\n",
    "    dists = np.sqrt(np.sum((X - X[0])**2, axis=1))\n",
    "    distances.append(dists[1:])\n",
    "\n",
    "# Plot the distribution of distances\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (d, dist) in enumerate(zip(dimensions, distances)):\n",
    "    axes[i].hist(dist, bins=50, alpha=0.7, density=True)\n",
    "    axes[i].set_title(f'{d}D space')\n",
    "    axes[i].set_xlabel('Distance')\n",
    "    axes[i].set_ylabel('Density')\n",
    "    \n",
    "plt.suptitle('Curse of Dimensionality: Distance Distributions', fontsize=16)\n",
    "plt.tight_layout()\n",
    "save_fig(\"curse_of_dimensionality\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate statistics\n",
    "print(\"Dimension | Mean Distance | Std Distance | Min/Max Ratio\")\n",
    "print(\"-\" * 60)\n",
    "for d, dist in zip(dimensions, distances):\n",
    "    mean_dist = np.mean(dist)\n",
    "    std_dist = np.std(dist)\n",
    "    min_max_ratio = np.min(dist) / np.max(dist)\n",
    "    print(f\"{d:9d} | {mean_dist:13.3f} | {std_dist:12.3f} | {min_max_ratio:13.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a04da5",
   "metadata": {},
   "source": [
    "## Main Approaches for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c8677",
   "metadata": {},
   "source": [
    "### Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D dataset that lies close to a 2D plane\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)\n",
    "\n",
    "# Visualize the 3D data\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "# 3D plot\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=angles, cmap=plt.cm.hot)\n",
    "ax1.set_xlabel('$x_1$')\n",
    "ax1.set_ylabel('$x_2$')\n",
    "ax1.set_zlabel('$x_3$')\n",
    "ax1.set_title('3D Dataset')\n",
    "\n",
    "# Projection onto x1-x2 plane\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=angles, cmap=plt.cm.hot)\n",
    "ax2.set_xlabel('$x_1$')\n",
    "ax2.set_ylabel('$x_2$')\n",
    "ax2.set_title('Projection onto $x_1$-$x_2$ plane')\n",
    "\n",
    "# Projection onto x1-x3 plane\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.scatter(X[:, 0], X[:, 2], c=angles, cmap=plt.cm.hot)\n",
    "ax3.set_xlabel('$x_1$')\n",
    "ax3.set_ylabel('$x_3$')\n",
    "ax3.set_title('Projection onto $x_1$-$x_3$ plane')\n",
    "\n",
    "save_fig(\"projection_example\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825bcd1",
   "metadata": {},
   "source": [
    "### Manifold Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc04791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swiss roll dataset - a classic manifold learning example\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "X_swiss, color = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "# 3D Swiss roll\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.scatter(X_swiss[:, 0], X_swiss[:, 1], X_swiss[:, 2], c=color, cmap=plt.cm.hot)\n",
    "ax1.set_xlabel('$x_1$')\n",
    "ax1.set_ylabel('$x_2$')\n",
    "ax1.set_zlabel('$x_3$')\n",
    "ax1.set_title('Swiss Roll (3D)')\n",
    "\n",
    "# Projection onto x1-x2 plane (bad)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.scatter(X_swiss[:, 0], X_swiss[:, 1], c=color, cmap=plt.cm.hot)\n",
    "ax2.set_xlabel('$x_1$')\n",
    "ax2.set_ylabel('$x_2$')\n",
    "ax2.set_title('Bad Projection')\n",
    "\n",
    "# Projection onto x1-x3 plane (better)\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.scatter(X_swiss[:, 0], X_swiss[:, 2], c=color, cmap=plt.cm.hot)\n",
    "ax3.set_xlabel('$x_1$')\n",
    "ax3.set_ylabel('$x_3$')\n",
    "ax3.set_title('Better Projection')\n",
    "\n",
    "save_fig(\"swiss_roll_projections\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8808ec8",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef760e",
   "metadata": {},
   "source": [
    "### Preserving the Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d057bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2D data\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "X_2d = np.random.randn(m, 2)\n",
    "X_2d = X_2d.dot(np.array([[1.5, 0], [0, 0.5]]))\n",
    "X_2d = X_2d.dot(np.array([[np.cos(np.pi/6), -np.sin(np.pi/6)], \n",
    "                          [np.sin(np.pi/6), np.cos(np.pi/6)]]))\n",
    "\n",
    "# Perform PCA manually\n",
    "X_centered = X_2d - X_2d.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]\n",
    "\n",
    "# Project data onto principal components\n",
    "W2 = Vt.T[:, :2]\n",
    "X2D_pca = X_centered.dot(W2)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Original data with principal components\n",
    "axes[0].scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.7)\n",
    "axes[0].arrow(X_2d.mean(0)[0], X_2d.mean(0)[1], c1[0]*3, c1[1]*3, \n",
    "              head_width=0.1, head_length=0.1, fc='red', ec='red')\n",
    "axes[0].arrow(X_2d.mean(0)[0], X_2d.mean(0)[1], c2[0]*3, c2[1]*3, \n",
    "              head_width=0.1, head_length=0.1, fc='blue', ec='blue')\n",
    "axes[0].set_xlabel('$x_1$')\n",
    "axes[0].set_ylabel('$x_2$')\n",
    "axes[0].set_title('Original Data with Principal Components')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Projected data\n",
    "axes[1].scatter(X2D_pca[:, 0], X2D_pca[:, 1], alpha=0.7)\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].set_title('Data Projected onto Principal Components')\n",
    "axes[1].grid(True)\n",
    "\n",
    "save_fig(\"pca_example\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Explained variance ratio: {s**2 / np.sum(s**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b109a698",
   "metadata": {},
   "source": [
    "### Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39819f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X_2d)\n",
    "\n",
    "print(f\"Principal components:\\n{pca.components_}\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Singular values: {pca.singular_values_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ffc8a",
   "metadata": {},
   "source": [
    "### Projecting Down to d Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f92340",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "X1D = pca.fit_transform(X_2d)\n",
    "print(f\"1D projection shape: {X1D.shape}\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "\n",
    "# Reconstruct the data\n",
    "X_recovered = pca.inverse_transform(X1D)\n",
    "\n",
    "# Visualize projection and reconstruction\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original data\n",
    "axes[0].scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.7, label='Original')\n",
    "axes[0].set_xlabel('$x_1$')\n",
    "axes[0].set_ylabel('$x_2$')\n",
    "axes[0].set_title('Original 2D Data')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# 1D projection\n",
    "axes[1].scatter(X1D[:, 0], np.zeros_like(X1D[:, 0]), alpha=0.7)\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('')\n",
    "axes[1].set_title('1D Projection')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Reconstructed data\n",
    "axes[2].scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.7, label='Original')\n",
    "axes[2].scatter(X_recovered[:, 0], X_recovered[:, 1], alpha=0.7, \n",
    "               marker='x', s=50, label='Reconstructed')\n",
    "for i in range(len(X_2d)):\n",
    "    axes[2].plot([X_2d[i, 0], X_recovered[i, 0]], \n",
    "                [X_2d[i, 1], X_recovered[i, 1]], 'k--', alpha=0.3)\n",
    "axes[2].set_xlabel('$x_1$')\n",
    "axes[2].set_ylabel('$x_2$')\n",
    "axes[2].set_title('Original vs Reconstructed')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "save_fig(\"pca_projection_reconstruction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfb347",
   "metadata": {},
   "source": [
    "### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec781a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.uint8)\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "print(f\"MNIST shape: {X.shape}\")\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "# Plot explained variance\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(pca.explained_variance_ratio_[:50])\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance per Component')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(cumsum[:200])\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "plt.axhline(y=0.99, color='g', linestyle='--', label='99% variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "save_fig(\"mnist_pca_variance\")\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for different variance levels\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "print(f\"Components needed for 95% variance: {d}\")\n",
    "d99 = np.argmax(cumsum >= 0.99) + 1\n",
    "print(f\"Components needed for 99% variance: {d99}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b881f670",
   "metadata": {},
   "source": [
    "### Choosing the Right Number of Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c321f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 95% variance preserved\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "print(f\"Reduced dimensions: {X_reduced.shape[1]}\")\n",
    "print(f\"Compression ratio: {X.shape[1] / X_reduced.shape[1]:.1f}\")\n",
    "\n",
    "# Reconstruct images and visualize\n",
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "\n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.extend([np.zeros((size, size))] * n_empty)\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# Display original vs reconstructed images\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "plt.subplot(211)\n",
    "plot_digits(X[:10])\n",
    "plt.title(\"Original Images\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plot_digits(X_recovered[:10])\n",
    "plt.title(f\"Reconstructed Images ({pca.n_components_} components)\")\n",
    "\n",
    "save_fig(\"mnist_pca_reconstruction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7502789a",
   "metadata": {},
   "source": [
    "### PCA for Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e470c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different compression levels\n",
    "components = [2, 10, 50, 154, 784]\n",
    "fig, axes = plt.subplots(1, len(components), figsize=(15, 3))\n",
    "\n",
    "# Use subset of data for faster computation\n",
    "X_sample = X[:1000]\n",
    "\n",
    "for i, n_comp in enumerate(components):\n",
    "    if n_comp == 784:\n",
    "        # Original image\n",
    "        X_rec = X_sample\n",
    "        title = \"Original\"\n",
    "    else:\n",
    "        pca = PCA(n_components=n_comp)\n",
    "        X_reduced = pca.fit_transform(X_sample)\n",
    "        X_rec = pca.inverse_transform(X_reduced)\n",
    "        compression_ratio = X_sample.shape[1] / n_comp\n",
    "        title = f\"{n_comp} components\\n({compression_ratio:.1f}x compression)\"\n",
    "    \n",
    "    plt.subplot(1, len(components), i + 1)\n",
    "    plot_digits(X_rec[0:1])\n",
    "    plt.title(title, fontsize=10)\n",
    "\n",
    "save_fig(\"pca_compression_comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e7eda",
   "metadata": {},
   "source": [
    "### Randomized PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e776917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Compare regular PCA vs randomized PCA\n",
    "X_sample = X[:1000]  # Use smaller sample for timing\n",
    "\n",
    "# Regular PCA\n",
    "start_time = time.time()\n",
    "pca_regular = PCA(n_components=154, svd_solver=\"full\")\n",
    "X_reduced_regular = pca_regular.fit_transform(X_sample)\n",
    "regular_time = time.time() - start_time\n",
    "\n",
    "# Randomized PCA\n",
    "start_time = time.time()\n",
    "pca_randomized = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced_randomized = pca_randomized.fit_transform(X_sample)\n",
    "randomized_time = time.time() - start_time\n",
    "\n",
    "print(f\"Regular PCA time: {regular_time:.3f} seconds\")\n",
    "print(f\"Randomized PCA time: {randomized_time:.3f} seconds\")\n",
    "print(f\"Speedup: {regular_time/randomized_time:.1f}x\")\n",
    "\n",
    "# Check if results are similar\n",
    "np.allclose(X_reduced_regular, X_reduced_randomized)\n",
    "print(f\"Results are approximately equal: {np.allclose(X_reduced_regular, X_reduced_randomized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ac650",
   "metadata": {},
   "source": [
    "### Incremental PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f960a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "# For large datasets that don't fit in memory\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "\n",
    "# Simulate batch processing\n",
    "X_sample = X[:1000]  # Use sample for demonstration\n",
    "batch_size = len(X_sample) // n_batches\n",
    "\n",
    "for i in range(n_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(X_sample))\n",
    "    inc_pca.partial_fit(X_sample[start_idx:end_idx])\n",
    "\n",
    "X_reduced_inc = inc_pca.transform(X_sample)\n",
    "X_recovered_inc = inc_pca.inverse_transform(X_reduced_inc)\n",
    "\n",
    "print(f\"Incremental PCA shape: {X_reduced_inc.shape}\")\n",
    "print(f\"Explained variance ratio sum: {inc_pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Compare with regular PCA\n",
    "pca_regular = PCA(n_components=154)\n",
    "X_reduced_reg = pca_regular.fit_transform(X_sample)\n",
    "X_recovered_reg = pca_regular.inverse_transform(X_reduced_reg)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 9))\n",
    "\n",
    "plt.subplot(311)\n",
    "plot_digits(X_sample[:5])\n",
    "plt.title(\"Original\")\n",
    "\n",
    "plt.subplot(312)\n",
    "plot_digits(X_recovered_reg[:5])\n",
    "plt.title(\"Regular PCA\")\n",
    "\n",
    "plt.subplot(313)\n",
    "plot_digits(X_recovered_inc[:5])\n",
    "plt.title(\"Incremental PCA\")\n",
    "\n",
    "save_fig(\"incremental_pca_comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8711674",
   "metadata": {},
   "source": [
    "## Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee12f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nonlinear dataset\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "X_swiss, color = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "# Apply different kernels\n",
    "kernels = ['linear', 'rbf', 'sigmoid']\n",
    "gammas = [None, 0.04, 1]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Original data\n",
    "ax = fig.add_subplot(2, 4, 1, projection='3d')\n",
    "ax.scatter(X_swiss[:, 0], X_swiss[:, 1], X_swiss[:, 2], c=color, cmap=plt.cm.hot)\n",
    "ax.set_title('Original Swiss Roll')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$x_3$')\n",
    "\n",
    "# Regular PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_swiss)\n",
    "axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=color, cmap=plt.cm.hot)\n",
    "axes[0, 1].set_title('Linear PCA')\n",
    "axes[0, 1].set_xlabel('PC1')\n",
    "axes[0, 1].set_ylabel('PC2')\n",
    "\n",
    "# Kernel PCA with different kernels\n",
    "for i, (kernel, gamma) in enumerate(zip(kernels, gammas)):\n",
    "    kpca = KernelPCA(n_components=2, kernel=kernel, gamma=gamma, random_state=42)\n",
    "    X_kpca = kpca.fit_transform(X_swiss)\n",
    "    \n",
    "    row = i // 2\n",
    "    col = (i % 2) + 2\n",
    "    axes[row, col].scatter(X_kpca[:, 0], X_kpca[:, 1], c=color, cmap=plt.cm.hot)\n",
    "    axes[row, col].set_title(f'Kernel PCA ({kernel})')\n",
    "    axes[row, col].set_xlabel('PC1')\n",
    "    axes[row, col].set_ylabel('PC2')\n",
    "\n",
    "# Hide unused subplot\n",
    "axes[1, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(\"kernel_pca_comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a9f53c",
   "metadata": {},
   "source": [
    "### Selecting a Kernel and Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a classification dataset\n",
    "from sklearn.datasets import make_moons\n",
    "X_moons, y_moons = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "# Pipeline with Kernel PCA and Logistic Regression\n",
    "clf = Pipeline([\n",
    "        (\"kpca\", KernelPCA(n_components=2)),\n",
    "        (\"log_reg\", LogisticRegression(solver=\"lbfgs\"))\n",
    "    ])\n",
    "\n",
    "param_grid = [{\n",
    "        \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "        \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "    }]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X_moons, y_moons)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb87921",
   "metadata": {},
   "source": [
    "## Locally Linear Embedding (LLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c7caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "# Apply LLE to Swiss Roll\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "X_lle = lle.fit_transform(X_swiss)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original Swiss Roll\n",
    "ax = fig.add_subplot(1, 3, 1, projection='3d')\n",
    "ax.scatter(X_swiss[:, 0], X_swiss[:, 1], X_swiss[:, 2], c=color, cmap=plt.cm.hot)\n",
    "ax.set_title('Original Swiss Roll')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$x_3$')\n",
    "\n",
    "# Linear PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_swiss)\n",
    "axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=color, cmap=plt.cm.hot)\n",
    "axes[1].set_title('Linear PCA')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "\n",
    "# LLE\n",
    "axes[2].scatter(X_lle[:, 0], X_lle[:, 1], c=color, cmap=plt.cm.hot)\n",
    "axes[2].set_title('LLE')\n",
    "axes[2].set_xlabel('Dimension 1')\n",
    "axes[2].set_ylabel('Dimension 2')\n",
    "\n",
    "save_fig(\"lle_comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa89f10",
   "metadata": {},
   "source": [
    "## Other Dimensionality Reduction Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e715641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS, TSNE, Isomap\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Use a smaller subset for faster computation\n",
    "X_subset = X_swiss[::2]  # Every other point\n",
    "color_subset = color[::2]\n",
    "\n",
    "# Apply different techniques\n",
    "techniques = {\n",
    "    'PCA': PCA(n_components=2),\n",
    "    'LLE': LocallyLinearEmbedding(n_components=2, n_neighbors=10),\n",
    "    'MDS': MDS(n_components=2, random_state=42),\n",
    "    'Isomap': Isomap(n_components=2),\n",
    "    't-SNE': TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original data\n",
    "ax = fig.add_subplot(2, 3, 1, projection='3d')\n",
    "ax.scatter(X_subset[:, 0], X_subset[:, 1], X_subset[:, 2], c=color_subset, cmap=plt.cm.hot)\n",
    "ax.set_title('Original Swiss Roll')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$x_3$')\n",
    "\n",
    "# Apply each technique\n",
    "for i, (name, technique) in enumerate(techniques.items()):\n",
    "    row = (i + 1) // 3\n",
    "    col = (i + 1) % 3\n",
    "    \n",
    "    try:\n",
    "        X_transformed = technique.fit_transform(X_subset)\n",
    "        axes[row, col].scatter(X_transformed[:, 0], X_transformed[:, 1], \n",
    "                              c=color_subset, cmap=plt.cm.hot)\n",
    "        axes[row, col].set_title(name)\n",
    "        axes[row, col].set_xlabel('Dimension 1')\n",
    "        axes[row, col].set_ylabel('Dimension 2')\n",
    "    except Exception as e:\n",
    "        axes[row, col].text(0.5, 0.5, f'Error:\\n{str(e)[:50]}...', \n",
    "                           ha='center', va='center', transform=axes[row, col].transAxes)\n",
    "        axes[row, col].set_title(f'{name} (Error)')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(\"dimensionality_reduction_comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a252ff0",
   "metadata": {},
   "source": [
    "## Performance Comparison on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6846be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison on MNIST subset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Use smaller subset for comparison\n",
    "X_mnist_subset = X[:2000]\n",
    "y_mnist_subset = y[:2000]\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_mnist_subset, y_mnist_subset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Baseline: No dimensionality reduction\n",
    "start_time = time.time()\n",
    "clf_baseline = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_baseline.fit(X_train, y_train)\n",
    "y_pred_baseline = clf_baseline.predict(X_test)\n",
    "baseline_time = time.time() - start_time\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "\n",
    "# PCA + Random Forest\n",
    "start_time = time.time()\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "clf_pca = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = clf_pca.predict(X_test_pca)\n",
    "pca_time = time.time() - start_time\n",
    "pca_accuracy = accuracy_score(y_test, y_pred_pca)\n",
    "\n",
    "# Results\n",
    "print(\"Performance Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Baseline (784 features):\")\n",
    "print(f\"  Accuracy: {baseline_accuracy:.3f}\")\n",
    "print(f\"  Time: {baseline_time:.2f} seconds\")\n",
    "print(f\"\\nPCA ({pca.n_components_} features):\")\n",
    "print(f\"  Accuracy: {pca_accuracy:.3f}\")\n",
    "print(f\"  Time: {pca_time:.2f} seconds\")\n",
    "print(f\"  Speedup: {baseline_time/pca_time:.1f}x\")\n",
    "print(f\"  Dimensionality reduction: {784/pca.n_components_:.1f}x\")\n",
    "\n",
    "# Visualize results\n",
    "methods = ['Baseline\\n(784 features)', f'PCA\\n({pca.n_components_} features)']\n",
    "accuracies = [baseline_accuracy, pca_accuracy]\n",
    "times = [baseline_time, pca_time]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].bar(methods, accuracies, color=['skyblue', 'lightgreen'])\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Classification Accuracy')\n",
    "axes[0].set_ylim([0.8, 1.0])\n",
    "\n",
    "axes[1].bar(methods, times, color=['skyblue', 'lightgreen'])\n",
    "axes[1].set_ylabel('Time (seconds)')\n",
    "axes[1].set_title('Training + Prediction Time')\n",
    "\n",
    "save_fig(\"pca_performance_comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b992180",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a034db",
   "metadata": {},
   "source": [
    "In this chapter, we covered dimensionality reduction techniques:\n",
    "\n",
    "### **Key Concepts:**\n",
    "\n",
    "1. **Curse of Dimensionality**\n",
    "   - Points become equidistant in high dimensions\n",
    "   - Data becomes sparse\n",
    "   - Computational complexity increases\n",
    "\n",
    "2. **Main Approaches**\n",
    "   - **Projection**: Project onto lower-dimensional space\n",
    "   - **Manifold Learning**: Unfold complex structures\n",
    "\n",
    "### **Techniques Covered:**\n",
    "\n",
    "1. **Principal Component Analysis (PCA)**\n",
    "   - Finds directions of maximum variance\n",
    "   - Linear technique, fast and simple\n",
    "   - Good for compression and noise reduction\n",
    "   - Variants: Randomized, Incremental PCA\n",
    "\n",
    "2. **Kernel PCA**\n",
    "   - Nonlinear extension of PCA\n",
    "   - Uses kernel trick for complex manifolds\n",
    "   - Good for nonlinear dimensionality reduction\n",
    "\n",
    "3. **Locally Linear Embedding (LLE)**\n",
    "   - Preserves local relationships\n",
    "   - Good for unfolding manifolds\n",
    "   - Nonlinear technique\n",
    "\n",
    "4. **Other Techniques**\n",
    "   - **MDS**: Preserves distances\n",
    "   - **Isomap**: Geodesic distances\n",
    "   - **t-SNE**: Great for visualization\n",
    "\n",
    "### **When to Use:**\n",
    "\n",
    "- **PCA**: First choice, data compression, noise reduction\n",
    "- **Kernel PCA**: Nonlinear relationships suspected\n",
    "- **LLE**: Manifold structure, local relationships important\n",
    "- **t-SNE**: Data visualization, clustering analysis\n",
    "\n",
    "### **Benefits:**\n",
    "\n",
    "- Faster training and prediction\n",
    "- Reduced storage requirements\n",
    "- Noise reduction\n",
    "- Visualization of high-dimensional data\n",
    "- Avoiding curse of dimensionality\n",
    "\n",
    "### **Best Practices:**\n",
    "\n",
    "- Always scale data before applying PCA\n",
    "- Choose components to retain 95-99% variance\n",
    "- Use cross-validation for hyperparameter tuning\n",
    "- Consider computational cost vs. performance gain\n",
    "- Validate that reduced dimensions preserve important information"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
