{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5310f813",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fdb6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957340bc",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c698b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('midterm-regresi-dataset.csv', header=None)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"First few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007be765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target and features\n",
    "# First column is the target (year), rest are features\n",
    "y = df.iloc[:, 0].values  # Target: release year\n",
    "X = df.iloc[:, 1:].values  # Features: audio characteristics\n",
    "\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"\\nNumber of features: {X.shape[1]}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70571fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the target variable\n",
    "print(\"Target Variable (Release Year) Statistics:\")\n",
    "print(f\"Mean: {y.mean():.2f}\")\n",
    "print(f\"Median: {np.median(y):.2f}\")\n",
    "print(f\"Std Dev: {y.std():.2f}\")\n",
    "print(f\"Min: {y.min():.0f}\")\n",
    "print(f\"Max: {y.max():.0f}\")\n",
    "print(f\"Range: {y.max() - y.min():.0f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba0628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(y, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Distribution of Release Years', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Year')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(y, vert=True)\n",
    "axes[1].set_title('Box Plot of Release Years', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Year')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Violin plot\n",
    "parts = axes[2].violinplot([y], vert=True, showmeans=True, showmedians=True)\n",
    "axes[2].set_title('Violin Plot of Release Years', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Year')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c8457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values in Dataset:\")\n",
    "print(f\"Target: {np.isnan(y).sum()}\")\n",
    "print(f\"Features: {np.isnan(X).sum()}\")\n",
    "\n",
    "if np.isnan(X).sum() > 0:\n",
    "    print(f\"\\nPercentage of missing values: {(np.isnan(X).sum() / X.size * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a0eef1",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for easier manipulation\n",
    "feature_names = [f'feature_{i+1}' for i in range(X.shape[1])]\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "y_series = pd.Series(y, name='year')\n",
    "\n",
    "print(\"Features DataFrame:\")\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb472838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of features\n",
    "print(\"Feature Statistics:\")\n",
    "X_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625f5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and handle outliers using IQR method\n",
    "def detect_outliers_iqr(data, threshold=1.5):\n",
    "    \"\"\"\n",
    "    Detect outliers using Interquartile Range (IQR) method\n",
    "    \"\"\"\n",
    "    Q1 = np.percentile(data, 25)\n",
    "    Q3 = np.percentile(data, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    \n",
    "    outliers = (data < lower_bound) | (data > upper_bound)\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Check for outliers in target variable\n",
    "outliers_y, lower_y, upper_y = detect_outliers_iqr(y)\n",
    "print(f\"Outliers in target variable: {outliers_y.sum()} ({outliers_y.sum()/len(y)*100:.2f}%)\")\n",
    "print(f\"Target range without outliers: [{lower_y:.0f}, {upper_y:.0f}]\")\n",
    "\n",
    "# For this task, we'll keep outliers as they represent valid years\n",
    "print(\"\\nNote: Keeping all data points as they represent valid release years.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dddff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions (sample of first 12 features)\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(12, X.shape[1])):\n",
    "    axes[i].hist(X[:, i], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_title(f'Feature {i+1}', fontsize=10)\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribution of First 12 Features', fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892489c1",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split and Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set year range: [{y_train.min():.0f}, {y_train.max():.0f}]\")\n",
    "print(f\"Test set year range: [{y_test.min():.0f}, {y_test.max():.0f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c91ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(f\"Scaled training features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test features shape: {X_test_scaled.shape}\")\n",
    "print(f\"\\nMean of scaled features (should be ~0): {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Std of scaled features (should be ~1): {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe3b2f",
   "metadata": {},
   "source": [
    "## 5. Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c0cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate regression models\n",
    "def evaluate_regression_model(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate a regression model\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"\\nTraining Set:\")\n",
    "    print(f\"  MSE:  {train_mse:.4f}\")\n",
    "    print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"  MAE:  {train_mae:.4f}\")\n",
    "    print(f\"  R²:   {train_r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTest Set:\")\n",
    "    print(f\"  MSE:  {test_mse:.4f}\")\n",
    "    print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"  MAE:  {test_mae:.4f}\")\n",
    "    print(f\"  R²:   {test_r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'model_name': model_name,\n",
    "        'train_mse': train_mse,\n",
    "        'test_mse': test_mse,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'y_train_pred': y_train_pred,\n",
    "        'y_test_pred': y_test_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35442ca",
   "metadata": {},
   "source": [
    "### 5.1 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96298753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_results = evaluate_regression_model(lr_model, X_train_scaled, y_train, X_test_scaled, y_test, \"Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e967099",
   "metadata": {},
   "source": [
    "### 5.2 Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression (L2 regularization)\n",
    "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_results = evaluate_regression_model(ridge_model, X_train_scaled, y_train, X_test_scaled, y_test, \"Ridge Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab2283b",
   "metadata": {},
   "source": [
    "### 5.3 Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b586096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression (L1 regularization)\n",
    "lasso_model = Lasso(alpha=0.1, random_state=42, max_iter=10000)\n",
    "lasso_results = evaluate_regression_model(lasso_model, X_train_scaled, y_train, X_test_scaled, y_test, \"Lasso Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a50871",
   "metadata": {},
   "source": [
    "### 5.4 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246c0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1)\n",
    "rf_results = evaluate_regression_model(rf_model, X_train_scaled, y_train, X_test_scaled, y_test, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd09962",
   "metadata": {},
   "source": [
    "### 5.5 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_results = evaluate_regression_model(xgb_model, X_train_scaled, y_train, X_test_scaled, y_test, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efa3ab",
   "metadata": {},
   "source": [
    "### 5.6 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "lgbm_model = LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "lgbm_results = evaluate_regression_model(lgbm_model, X_train_scaled, y_train, X_test_scaled, y_test, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcbf1d5",
   "metadata": {},
   "source": [
    "## 6. Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe4ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Neural Network for Regression\n",
    "def build_regression_nn(input_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(256, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Dense(1)  # Output layer for regression (no activation)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae', keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "nn_model = build_regression_nn(X_train_scaled.shape[1])\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0502bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=7,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Neural Network...\")\n",
    "history = nn_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e2757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_title('Model Loss (MSE)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Train MAE')\n",
    "axes[1].plot(history.history['val_mae'], label='Val MAE')\n",
    "axes[1].set_title('Mean Absolute Error', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# RMSE\n",
    "axes[2].plot(history.history['rmse'], label='Train RMSE')\n",
    "axes[2].plot(history.history['val_rmse'], label='Val RMSE')\n",
    "axes[2].set_title('Root Mean Squared Error', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('RMSE')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec86d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Neural Network\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evaluating Neural Network...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_nn = nn_model.predict(X_train_scaled).flatten()\n",
    "y_test_pred_nn = nn_model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "train_mse_nn = mean_squared_error(y_train, y_train_pred_nn)\n",
    "test_mse_nn = mean_squared_error(y_test, y_test_pred_nn)\n",
    "train_rmse_nn = np.sqrt(train_mse_nn)\n",
    "test_rmse_nn = np.sqrt(test_mse_nn)\n",
    "train_mae_nn = mean_absolute_error(y_train, y_train_pred_nn)\n",
    "test_mae_nn = mean_absolute_error(y_test, y_test_pred_nn)\n",
    "train_r2_nn = r2_score(y_train, y_train_pred_nn)\n",
    "test_r2_nn = r2_score(y_test, y_test_pred_nn)\n",
    "\n",
    "print(f\"\\nNeural Network Results:\")\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  MSE:  {train_mse_nn:.4f}\")\n",
    "print(f\"  RMSE: {train_rmse_nn:.4f}\")\n",
    "print(f\"  MAE:  {train_mae_nn:.4f}\")\n",
    "print(f\"  R²:   {train_r2_nn:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  MSE:  {test_mse_nn:.4f}\")\n",
    "print(f\"  RMSE: {test_rmse_nn:.4f}\")\n",
    "print(f\"  MAE:  {test_mae_nn:.4f}\")\n",
    "print(f\"  R²:   {test_r2_nn:.4f}\")\n",
    "\n",
    "# Store results\n",
    "nn_results = {\n",
    "    'model': nn_model,\n",
    "    'model_name': 'Neural Network',\n",
    "    'train_mse': train_mse_nn,\n",
    "    'test_mse': test_mse_nn,\n",
    "    'train_rmse': train_rmse_nn,\n",
    "    'test_rmse': test_rmse_nn,\n",
    "    'train_mae': train_mae_nn,\n",
    "    'test_mae': test_mae_nn,\n",
    "    'train_r2': train_r2_nn,\n",
    "    'test_r2': test_r2_nn,\n",
    "    'y_train_pred': y_train_pred_nn,\n",
    "    'y_test_pred': y_test_pred_nn\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf2890",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = [lr_results, ridge_results, lasso_results, rf_results, xgb_results, lgbm_results, nn_results]\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['model_name'],\n",
    "        'Train RMSE': result['train_rmse'],\n",
    "        'Test RMSE': result['test_rmse'],\n",
    "        'Train MAE': result['train_mae'],\n",
    "        'Test MAE': result['test_mae'],\n",
    "        'Train R²': result['train_r2'],\n",
    "        'Test R²': result['test_r2']\n",
    "    }\n",
    "    for result in all_results\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Find best model based on Test R²\n",
    "best_model_idx = comparison_df['Test R²'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\nBest Model (by Test R²): {best_model_name}\")\n",
    "print(f\"Test R²: {comparison_df.loc[best_model_idx, 'Test R²']:.4f}\")\n",
    "print(f\"Test RMSE: {comparison_df.loc[best_model_idx, 'Test RMSE']:.4f} years\")\n",
    "print(f\"Test MAE: {comparison_df.loc[best_model_idx, 'Test MAE']:.4f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "metrics = [('Train RMSE', 'Test RMSE'), ('Train MAE', 'Test MAE'), ('Train R²', 'Test R²')]\n",
    "\n",
    "for idx, (train_metric, test_metric) in enumerate(metrics):\n",
    "    # Training metrics\n",
    "    row = 0\n",
    "    col = idx\n",
    "    axes[row, col].bar(comparison_df['Model'], comparison_df[train_metric], color='steelblue')\n",
    "    axes[row, col].set_title(f'{train_metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    axes[row, col].set_ylabel(train_metric)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(comparison_df[train_metric]):\n",
    "        axes[row, col].text(i, v, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Test metrics\n",
    "    row = 1\n",
    "    axes[row, col].bar(comparison_df['Model'], comparison_df[test_metric], color='salmon')\n",
    "    axes[row, col].set_title(f'{test_metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    axes[row, col].set_ylabel(test_metric)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(comparison_df[test_metric]):\n",
    "        axes[row, col].text(i, v, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe60ff",
   "metadata": {},
   "source": [
    "## 8. Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model predictions\n",
    "best_result = all_results[best_model_idx]\n",
    "y_test_pred_best = best_result['y_test_pred']\n",
    "\n",
    "# Scatter plot: Actual vs Predicted\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, y_test_pred_best, alpha=0.5, color='steelblue')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Year', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Year', fontsize=12)\n",
    "axes[0].set_title(f'Actual vs Predicted ({best_model_name})', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_test_pred_best\n",
    "axes[1].scatter(y_test_pred_best, residuals, alpha=0.5, color='salmon')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Year', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[1].set_title(f'Residual Plot ({best_model_name})', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual statistics\n",
    "print(f\"\\nResidual Analysis for {best_model_name}:\")\n",
    "print(f\"Mean Residual: {residuals.mean():.4f}\")\n",
    "print(f\"Std Residual: {residuals.std():.4f}\")\n",
    "print(f\"Min Residual: {residuals.min():.4f}\")\n",
    "print(f\"Max Residual: {residuals.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e912fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of prediction errors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Histogram of residuals\n",
    "axes[0].hist(residuals, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[0].set_xlabel('Residual (years)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Residuals', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot for normality check\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot (Normality Check)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions across all models\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, result in enumerate(all_results):\n",
    "    y_pred = result['y_test_pred']\n",
    "    axes[idx].scatter(y_test, y_pred, alpha=0.5)\n",
    "    axes[idx].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    axes[idx].set_xlabel('Actual Year')\n",
    "    axes[idx].set_ylabel('Predicted Year')\n",
    "    axes[idx].set_title(f\"{result['model_name']}\\n(R²={result['test_r2']:.3f}, RMSE={result['test_rmse']:.2f})\")\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide the last subplot\n",
    "axes[7].axis('off')\n",
    "\n",
    "plt.suptitle('Actual vs Predicted - All Models', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0fa3b",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c0cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "if 'Random Forest' in [r['model_name'] for r in all_results]:\n",
    "    rf_idx = [r['model_name'] for r in all_results].index('Random Forest')\n",
    "    rf_model_trained = all_results[rf_idx]['model']\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': rf_model_trained.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_20 = feature_importance.head(20)\n",
    "    plt.barh(range(len(top_20)), top_20['importance'], color='steelblue')\n",
    "    plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.title('Top 20 Most Important Features (Random Forest)', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 20 Most Important Features:\")\n",
    "    print(feature_importance.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19fcb3d",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762080d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SONG YEAR PREDICTION PROJECT SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n1. Dataset Statistics:\")\n",
    "print(f\"   - Total samples: {len(y)}\")\n",
    "print(f\"   - Number of features: {X.shape[1]}\")\n",
    "print(f\"   - Year range: {y.min():.0f} - {y.max():.0f}\")\n",
    "print(f\"   - Training set: {len(y_train)} samples\")\n",
    "print(f\"   - Test set: {len(y_test)} samples\")\n",
    "\n",
    "print(f\"\\n2. Models Evaluated:\")\n",
    "for i, result in enumerate(all_results, 1):\n",
    "    print(f\"   {i}. {result['model_name']}\")\n",
    "\n",
    "print(f\"\\n3. Best Model: {best_model_name}\")\n",
    "print(f\"   - Test R²: {comparison_df.loc[best_model_idx, 'Test R²']:.4f}\")\n",
    "print(f\"   - Test RMSE: {comparison_df.loc[best_model_idx, 'Test RMSE']:.4f} years\")\n",
    "print(f\"   - Test MAE: {comparison_df.loc[best_model_idx, 'Test MAE']:.4f} years\")\n",
    "print(f\"   - Interpretation: On average, predictions are off by ~{comparison_df.loc[best_model_idx, 'Test MAE']:.1f} years\")\n",
    "\n",
    "print(f\"\\n4. Key Techniques Used:\")\n",
    "print(f\"   - Feature scaling (StandardScaler)\")\n",
    "print(f\"   - Multiple regression algorithms (Linear, Ridge, Lasso, RF, XGBoost, LightGBM)\")\n",
    "print(f\"   - Deep Learning (Neural Network with batch normalization and dropout)\")\n",
    "print(f\"   - Comprehensive evaluation metrics (MSE, RMSE, MAE, R²)\")\n",
    "print(f\"   - Residual analysis and visualization\")\n",
    "\n",
    "print(f\"\\n5. Model Performance Ranking (by Test R²):\")\n",
    "ranked_df = comparison_df.sort_values('Test R²', ascending=False).reset_index(drop=True)\n",
    "for i, row in ranked_df.iterrows():\n",
    "    print(f\"   {i+1}. {row['Model']:20s} - R²: {row['Test R²']:.4f}, RMSE: {row['Test RMSE']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
