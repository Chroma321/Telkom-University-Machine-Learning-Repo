{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4776ca3",
   "metadata": {},
   "source": [
    "# Chapter 6 - Decision Trees\n",
    "\n",
    "This notebook covers Decision Trees, including:\n",
    "- Training and Visualizing Decision Trees\n",
    "- Making Predictions\n",
    "- Estimating Class Probabilities\n",
    "- The CART Training Algorithm\n",
    "- Computational Complexity\n",
    "- Gini Impurity vs Entropy\n",
    "- Regularization Hyperparameters\n",
    "- Regression Trees\n",
    "- Instability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53a55f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"decision_trees\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b3b234",
   "metadata": {},
   "source": [
    "## Training and Visualizing a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6911d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0445920a",
   "metadata": {},
   "source": [
    "You can visualize the trained Decision Tree by first using the `export_graphviz()` method to output a graph definition file called *iris_tree.dot*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710eae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=os.path.join(IMAGES_PATH, \"iris_tree.dot\"),\n",
    "        feature_names=iris.feature_names[2:],\n",
    "        class_names=iris.target_names,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c33b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple text representation\n",
    "from sklearn.tree import export_text\n",
    "r = export_text(tree_clf, feature_names=iris.feature_names[2:])\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b94d2",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the tree makes predictions\n",
    "print(\"Tree structure:\")\n",
    "print(f\"Root node checks: petal length <= 2.45\")\n",
    "print(f\"Left child (setosa): gini=0.0, samples=50\")\n",
    "print(f\"Right child splits on: petal width <= 1.75\")\n",
    "print(f\"Final prediction depends on the path taken\")\n",
    "\n",
    "# Test prediction\n",
    "print(f\"\\nPrediction for sample with petal length=5, petal width=1.5:\")\n",
    "print(f\"Predicted class: {tree_clf.predict([[5, 1.5]])}\")\n",
    "print(f\"Predicted probabilities: {tree_clf.predict_proba([[5, 1.5]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e67034",
   "metadata": {},
   "source": [
    "## Decision Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b42056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if not iris:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    if plot_training:\n",
    "        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\n",
    "        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")\n",
    "        plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"g^\", label=\"Iris virginica\")\n",
    "        plt.axis(axes)\n",
    "    if iris:\n",
    "        plt.xlabel(\"Petal length\", fontsize=14)\n",
    "        plt.ylabel(\"Petal width\", fontsize=14)\n",
    "    else:\n",
    "        plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "        plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "    if legend:\n",
    "        plt.legend(loc=\"lower right\", fontsize=14)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundary(tree_clf, X, y)\n",
    "plt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\n",
    "plt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n",
    "plt.text(1.40, 0.8, \"Depth=0\\ngini=0.667\\nsamples=150\\nvalue=[50, 50, 50]\", fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.5))\n",
    "plt.text(4.2, 1.2, \"Depth=1\\ngini=0.5\\nsamples=100\\nvalue=[0, 50, 50]\", fontsize=12, bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.5))\n",
    "plt.title(\"Decision Tree decision boundaries\", fontsize=14)\n",
    "save_fig(\"decision_tree_decision_boundaries_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f93b48",
   "metadata": {},
   "source": [
    "## Estimating Class Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39fdccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree_clf.predict_proba([[5, 1.5]]))\n",
    "print(tree_clf.predict([[5, 1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b742d",
   "metadata": {},
   "source": [
    "## The CART Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f097729",
   "metadata": {},
   "source": [
    "Scikit-Learn uses the *Classification And Regression Tree* (CART) algorithm to train Decision Trees. The algorithm works by first splitting the training set in two subsets using a single feature $k$ and a threshold $t_k$ (e.g., \"petal length ≤ 2.45 cm\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358c098",
   "metadata": {},
   "source": [
    "## Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f3ee5",
   "metadata": {},
   "source": [
    "Making predictions requires traversing the Decision Tree from the root to a leaf. Decision Trees are generally approximately balanced, so traversing the Decision Tree requires going through roughly $O(\\log_2(m))$ nodes.\n",
    "\n",
    "The training algorithm compares all features (or less if `max_features` is set) on all samples at each node. This results in a training complexity of $O(n × m \\log(m))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4f18ee",
   "metadata": {},
   "source": [
    "## Gini Impurity or Entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b973020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Gini and Entropy\n",
    "tree_clf_gini = DecisionTreeClassifier(max_depth=2, criterion=\"gini\", random_state=42)\n",
    "tree_clf_entropy = DecisionTreeClassifier(max_depth=2, criterion=\"entropy\", random_state=42)\n",
    "\n",
    "tree_clf_gini.fit(X, y)\n",
    "tree_clf_entropy.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_decision_boundary(tree_clf_gini, X, y, legend=False)\n",
    "plt.title(\"Gini\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_decision_boundary(tree_clf_entropy, X, y, legend=True)\n",
    "plt.title(\"Entropy\", fontsize=14)\n",
    "\n",
    "save_fig(\"gini_vs_entropy_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9601582",
   "metadata": {},
   "source": [
    "Let's compare the impurity measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ad5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Gini impurity and Entropy\n",
    "def gini(p):\n",
    "    return 1 - sum([pi**2 for pi in p])\n",
    "\n",
    "def entropy(p):\n",
    "    return -sum([pi * np.log2(pi) if pi > 0 else 0 for pi in p])\n",
    "\n",
    "def classification_error(p):\n",
    "    return 1 - max(p)\n",
    "\n",
    "x = np.linspace(0.01, 0.99, 100)\n",
    "gini_values = [gini([p, 1-p]) for p in x]\n",
    "entropy_values = [entropy([p, 1-p]) for p in x]\n",
    "error_values = [classification_error([p, 1-p]) for p in x]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, gini_values, \"b-\", linewidth=2, label=\"Gini\")\n",
    "plt.plot(x, entropy_values, \"r--\", linewidth=2, label=\"Entropy\")\n",
    "plt.plot(x, error_values, \"g:\", linewidth=2, label=\"Misclassification Rate\")\n",
    "plt.xlabel(\"Class 1 probability\")\n",
    "plt.ylabel(\"Impurity\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Impurity Measures\")\n",
    "plt.grid(True)\n",
    "save_fig(\"impurity_measures_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb1ec93",
   "metadata": {},
   "source": [
    "## Regularization Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f006f1ba",
   "metadata": {},
   "source": [
    "Decision Trees make very few assumptions about the training data. If left unconstrained, the tree structure will adapt itself to the training data, fitting it very closely, and most likely overfitting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7050058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "Xm, ym = make_moons(n_samples=100, noise=0.25, random_state=53)\n",
    "\n",
    "deep_tree_clf1 = DecisionTreeClassifier(random_state=42)\n",
    "deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4, random_state=42)\n",
    "deep_tree_clf1.fit(Xm, ym)\n",
    "deep_tree_clf2.fit(Xm, ym)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(deep_tree_clf1, Xm, ym, axes=[-1.5, 2.4, -1, 1.5], iris=False)\n",
    "plt.title(\"No restrictions\", fontsize=14)\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(deep_tree_clf2, Xm, ym, axes=[-1.5, 2.4, -1, 1.5], iris=False)\n",
    "plt.title(\"min_samples_leaf = 4\", fontsize=14)\n",
    "plt.ylabel(\"\")\n",
    "save_fig(\"min_samples_leaf_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92ff1b",
   "metadata": {},
   "source": [
    "Other hyperparameters control the shape of the Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d993de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different regularization parameters\n",
    "angle = np.pi / 180 * 20\n",
    "rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n",
    "Xr = X.dot(rotation_matrix)\n",
    "\n",
    "tree_clf_s = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_sr = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_s.fit(X, y)\n",
    "tree_clf_sr.fit(Xr, y)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 3), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(tree_clf_s, X, y, axes=[0.5, 7, 0, 3], iris=True)\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(tree_clf_sr, Xr, y, axes=[0.5, 7, 0, 3], iris=True)\n",
    "plt.ylabel(\"\")\n",
    "save_fig(\"tree_instability_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67d3d5",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7632bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic training set + noise\n",
    "np.random.seed(42)\n",
    "m = 200\n",
    "X = np.random.rand(m, 1)\n",
    "y = 4 * (X - 0.5) ** 2\n",
    "y = y + np.random.randn(m, 1) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8620d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0705fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, max_depth=3)\n",
    "tree_reg1.fit(X, y)\n",
    "tree_reg2.fit(X, y)\n",
    "\n",
    "def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=\"$y$\"):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n",
    "    y_pred = tree_reg.predict(x1)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel, fontsize=18, rotation=0)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_regression_predictions(tree_reg1, X, y)\n",
    "for split, style in [(0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")]:\n",
    "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "plt.text(0.21, 0.65, \"Depth=0\", fontsize=15)\n",
    "plt.text(0.01, 0.2, \"Depth=1\", fontsize=13)\n",
    "plt.text(0.65, 0.8, \"Depth=1\", fontsize=13)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"max_depth=2\", fontsize=14)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_regression_predictions(tree_reg2, X, y, ylabel=None)\n",
    "for split, style in [(0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")]:\n",
    "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "for split in [0.0458, 0.1298, 0.2873, 0.9040]:\n",
    "    plt.plot([split, split], [-0.2, 1], \"k:\", linewidth=1)\n",
    "plt.title(\"max_depth=3\", fontsize=14)\n",
    "\n",
    "save_fig(\"tree_regression_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b98175",
   "metadata": {},
   "source": [
    "## Tree Visualization with Export Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ab040",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)\n",
    "tree_reg1.fit(X, y)\n",
    "print(export_text(tree_reg1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03988e13",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)\n",
    "tree_reg1.fit(X, y)\n",
    "tree_reg2.fit(X, y)\n",
    "\n",
    "x1 = np.linspace(0, 1, 500).reshape(-1, 1)\n",
    "y_pred1 = tree_reg1.predict(x1)\n",
    "y_pred2 = tree_reg2.predict(x1)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", fontsize=18, rotation=0)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"No restrictions\", fontsize=14)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.title(\"min_samples_leaf=10\", fontsize=14)\n",
    "\n",
    "save_fig(\"tree_regression_regularization_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381dcd8",
   "metadata": {},
   "source": [
    "## Instability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "ys = np.array([0, 0, 1, 1])\n",
    "\n",
    "angle = np.pi / 4\n",
    "rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n",
    "Xsr = Xs.dot(rotation_matrix)\n",
    "\n",
    "tree_clf_s = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_sr = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_s.fit(Xs, ys)\n",
    "tree_clf_sr.fit(Xsr, ys)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(tree_clf_s, Xs, ys, axes=[-1.2, 1.2, -1.2, 1.2], iris=False)\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(tree_clf_sr, Xsr, ys, axes=[-1.2, 1.2, -1.2, 1.2], iris=False)\n",
    "plt.ylabel(\"\")\n",
    "save_fig(\"sensitivity_to_rotation_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487b0eee",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(iris.data, iris.target)\n",
    "\n",
    "print(\"Feature importances:\")\n",
    "for name, score in zip(iris.feature_names, tree_clf.feature_importances_):\n",
    "    print(f\"{name}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(8, 6))\n",
    "feature_names = iris.feature_names\n",
    "importances = tree_clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.bar(range(len(importances)), importances[indices])\n",
    "plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.tight_layout()\n",
    "save_fig(\"feature_importance_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d5b0f",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3fc77",
   "metadata": {},
   "source": [
    "In this chapter, we covered:\n",
    "\n",
    "1. **Decision Tree Fundamentals**: How trees make decisions using feature thresholds\n",
    "2. **CART Algorithm**: The training process for classification and regression trees\n",
    "3. **Impurity Measures**: Gini impurity vs entropy for splitting criteria\n",
    "4. **Regularization**: Controlling overfitting with hyperparameters like max_depth, min_samples_leaf\n",
    "5. **Regression Trees**: Applying decision trees to continuous target variables\n",
    "6. **Instability**: Understanding sensitivity to data changes and rotations\n",
    "7. **Feature Importance**: Quantifying which features are most useful\n",
    "\n",
    "**Key Advantages:**\n",
    "- Simple to understand and interpret\n",
    "- Requires little data preparation\n",
    "- Can handle both numerical and categorical data\n",
    "- Can handle multi-output problems\n",
    "- White box model (interpretable)\n",
    "- Can validate using statistical tests\n",
    "\n",
    "**Key Limitations:**\n",
    "- Prone to overfitting (especially deep trees)\n",
    "- Can be unstable (small data changes → different trees)\n",
    "- Biased toward features with more levels\n",
    "- Difficulty capturing linear relationships\n",
    "- Can create overly complex trees\n",
    "\n",
    "**Best Practices:**\n",
    "- Use regularization parameters (max_depth, min_samples_leaf, etc.)\n",
    "- Consider ensemble methods (Random Forest, Gradient Boosting)\n",
    "- Validate using cross-validation\n",
    "- Visualize trees to understand decision logic\n",
    "- Check feature importance for insights"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
